{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83deedd-96ed-4b4d-98d5-a155dd674a7f",
   "metadata": {},
   "source": [
    "## User and environment definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9590be-8490-4ff3-9b92-45fa16daef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file='../../ftransfer_ztf_2024-02-01_689626'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22166c0f-26ba-479d-b41e-2e86af3d0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -d $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604bc63c-a0d8-4dee-82e8-cbf72bab6ef2",
   "metadata": {},
   "source": [
    "## Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ca0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003db791-8f9a-4688-b52e-e222d8fb2715",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f8385",
   "metadata": {},
   "source": [
    "# 1) Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68831f6",
   "metadata": {},
   "source": [
    "Initially, we acquired the data from FINK using the link provided: https://fink-portal.org/download.\n",
    "\n",
    "#### Now, let's import the data into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6810444",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_parquet(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0005021",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb204e6",
   "metadata": {},
   "source": [
    "#### Here we calculate the first and last days on which an alert was recorded across all the data. \n",
    "we're identifying the earliest and latest dates observed across all the alerts in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_series = pdf.candidate.apply(lambda a:a['jd'])\n",
    "last_day = jd_series.max()\n",
    "\n",
    "first_day = np.min(pdf.prv_candidates.apply(lambda a: a[0]['jd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1de736-2dee-46f4-b687-095d320c93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day, first_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8df7d-19cc-4d4a-bfd1-7846ec7c204c",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228083e5",
   "metadata": {},
   "source": [
    "# 2) Select alert and build the data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207add8",
   "metadata": {},
   "source": [
    "### We select alerts (data) based on their shared ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = pdf['objectId'][4771]\n",
    "\n",
    "id_most_repeated = pdf['objectId'].value_counts().idxmax()\n",
    "\n",
    "Id = \"ZTF18acoyqvk\" # max by j (for all Q) here we got = 119 on idx = 13 (all Q matched)\n",
    "# Id = \"ZTF18abosdhe\" #min by j ,  ( idx = 5 , with one Q )\n",
    "\n",
    "#Id = \"ZTF18adarfuu\" # max by Q idx = 6\n",
    "Id = \"ZTF18aaxyrcb\" # min by Q idx = 7\n",
    "Id = \"ZTF19abizgyw\" # <10 data \n",
    "\n",
    "Id = \"ZTF18acckcza\" # Anomaly data analysis \n",
    "Id = \"ZTF18acevrat\" # probUpperlim\n",
    "Id = \"ZTF17aaafgst\" # prob stop\n",
    "Id = \"ZTF17aacezky\"\n",
    "Id = \"ZTF18abhqeds\" # no dc_mag ! \n",
    "Id = \"ZTF17aaaenex\" # ratio 35 we still have upp prob\n",
    "Id = \"ZTF17aabunxe\"\n",
    "Id = \"ZTF18aawawpd\" #prob only 1 red \n",
    "Id = \"ZTF18abcpbbw\" # strange warrnning /\n",
    "Id = \"ZTF18acgzsug\"\n",
    "Id = \"ZTF18aabeypj\" # middele \n",
    "Id =  \"ZTF18abwpcbt\" #problem in GP ! wrong values \n",
    "# Id = \"ZTF20abvtozi\" #Sn\n",
    "# Id = \"ZTF20abvrxkz\" #SN problem in GP ! wrong values \n",
    "pdf_filter_by_shared_Id = pdf.loc[pdf['objectId'] == Id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c0c24",
   "metadata": {},
   "source": [
    "### Choose the last alert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df = pdf_filter_by_shared_Id['candidate'].apply(pd.Series)\n",
    "\n",
    "candidate_df = candidate_df.sort_values(by= 'jd')\n",
    "index_max_jd = candidate_df.index[-1]\n",
    "\n",
    "# select this candidate\n",
    "pdf_last_alert = pdf_filter_by_shared_Id.loc[index_max_jd]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c878ac",
   "metadata": {},
   "source": [
    "### Transform the data of all candidates (including `prv_candidates` and the last one) into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd821f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_selec_cands = pdf_last_alert['prv_candidates'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd041afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  add 'candidate' the actual value \n",
    "keys = pdf_selec_cands[0].keys()\n",
    "latest_cand = {key: pdf_last_alert['candidate'][key] for key in keys if key in pdf_last_alert['candidate']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2516f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_dicts = list(pdf_selec_cands)\n",
    "liste_dicts.append(latest_cand)\n",
    "df = pd.DataFrame(liste_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb6079",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4c784",
   "metadata": {},
   "source": [
    "# 3) plot Difference Magnitude in Modified Julian Date [UTC]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd102bb",
   "metadata": {},
   "source": [
    "_Circles (●) with error bars show valid alerts that pass the Fink quality cuts.\n",
    "In addition, the Difference magnitude view shows:\n",
    "\n",
    "_upper triangles with errors (▲), representing alert measurements that do not satisfy Fink quality cuts, but are nevetheless contained in the history of valid alerts and used by classifiers.\n",
    "\n",
    "_lower triangles (▽), representing 5-sigma magnitude limit in difference image based on PSF-fit photometry contained in the history of valid alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "colordic = {1: 'C0', 2: 'C1'}\n",
    "filtdic = {1: 'g', 2: 'r'}\n",
    "\n",
    "# valid values \n",
    "maskValid = (df['rb'] >= 0.55) & (df['nbad'] == 0) #& (abs(df['magdiff']) <= 0.1)# magdiff = magap - magpsf\n",
    "# Upper limit values\n",
    "maskUpper = pd.isna(df['magpsf'])\n",
    "#bad quality values \n",
    "maskBadquality = ~maskValid & ~maskUpper\n",
    "\n",
    "for filt in np.unique(df['fid']):\n",
    "    maskFilt = df['fid'] == filt\n",
    "\n",
    "    plt.errorbar(\n",
    "        df[maskValid & maskFilt]['jd'].apply(lambda x: x - 2400000.5),\n",
    "        df[maskValid & maskFilt]['magpsf'],\n",
    "        df[maskValid & maskFilt]['sigmapsf'],\n",
    "        ls = '', marker='o', color=colordic[filt], label='{} band'.format(filtdic[filt])\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        df[maskUpper & maskFilt]['jd'].apply(lambda x: x - 2400000.5),\n",
    "        df[maskUpper & maskFilt]['diffmaglim'],\n",
    "        ls='', marker='v', color=colordic[filt], markerfacecolor='none'\n",
    "    )\n",
    "    \n",
    "\n",
    "    plt.errorbar(\n",
    "        df[maskBadquality & maskFilt]['jd'].apply(lambda x: x - 2400000.5),\n",
    "        df[maskBadquality & maskFilt]['magpsf'],\n",
    "        df[maskBadquality & maskFilt]['sigmapsf'],\n",
    "        ls='', marker='^', color=colordic[filt]\n",
    "    )\n",
    "\n",
    "#plt.ylim(14, 18)\n",
    "#plt.xlim(59084, 59086)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "plt.title('Difference image PSF-fit magnitude')\n",
    "plt.xlabel('Modified Julian Date [UTC]')\n",
    "plt.ylabel('Difference Magnitude');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35bf46",
   "metadata": {},
   "source": [
    "# Plot the magnitude difference for valid data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34ecf0",
   "metadata": {},
   "source": [
    "Distinguishing between positive differences represented by circles and negative differences represented by triangles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "colordic = {1: 'C0', 2: 'C1'}\n",
    "filtdic = {1: 'g', 2: 'r'}\n",
    "    \n",
    "maskValid = (df['rb'] >= 0.55) & (df['nbad'] == 0)\n",
    "\n",
    "    \n",
    "#t or 1 => candidate is from positive (sci minus ref) subtraction;\n",
    "#f or 0 => candidate is from negative (ref minus sci) subtraction\"\n",
    "maskpos = (df['isdiffpos'] == 't') | (df['isdiffpos'] == '1')\n",
    "maskneg = (df['isdiffpos'] == 'f') | (df['isdiffpos'] == '0')\n",
    "\n",
    "for filt in np.unique(df['fid']):\n",
    "    maskFilt = df['fid'] == filt\n",
    "\n",
    "    # candidates from negative \n",
    "\n",
    "    plt.errorbar(\n",
    "        df[maskValid & maskFilt & maskneg ]['jd'].apply(lambda x: x - 2400000.5),\n",
    "        df[maskValid & maskFilt & maskneg ]['magpsf'],\n",
    "        df[maskValid & maskFilt & maskneg ]['sigmapsf'],\n",
    "        ls = '', marker='^', color=colordic[filt], label='{} (-)'.format(filtdic[filt])\n",
    "    )\n",
    "    \n",
    "    # candidates from positive \n",
    "    plt.errorbar(\n",
    "        df[maskValid & maskFilt &  maskpos ]['jd'].apply(lambda x: x - 2400000.5),\n",
    "        df[maskValid & maskFilt &  maskpos ]['magpsf'],\n",
    "        df[maskValid & maskFilt &  maskpos ]['sigmapsf'],\n",
    "        ls = '', marker='o', color=colordic[filt], label='{} (+)'.format(filtdic[filt])\n",
    "    )\n",
    "    \n",
    "\n",
    "#plt.ylim(12, 22)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "plt.title('Difference image PSF-fit magnitude')\n",
    "plt.xlabel('Modified Julian Date [UTC]')\n",
    "plt.ylabel('Difference Magnitude');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7320a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9302975",
   "metadata": {},
   "source": [
    "# 4) Calculate and plot Apparent DC flux for the valid data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c69e3",
   "metadata": {},
   "source": [
    "We utilize a function(`apparent_flux_New`) located within our `FAnomAly` package to compute the apparent flux for the valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec900ed2-04fa-4bb1-88a9-b56960ed67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fink_utils.photometry.utils import is_source_behind\n",
    "from FAnomAly.flux import apparent_flux_New\n",
    "\n",
    "# Take only valid measurements\n",
    "maskValid = (df['rb'] >= 0.55) & (df['nbad'] == 0)\n",
    "df_valid = df[maskValid].sort_values('jd')\n",
    "df_valid['is_Source'] = is_source_behind(df_valid['distnr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "dc_flux, dc_sigflux = np.transpose(\n",
    "        [\n",
    "            apparent_flux_New(*args, jansky=True) for args in zip(\n",
    "                df_valid['magpsf'].astype(float).values,\n",
    "                df_valid['sigmapsf'].astype(float).values,\n",
    "                df_valid['magnr'].astype(float).values,\n",
    "                df_valid['sigmagnr'].astype(float).values,\n",
    "                df_valid['isdiffpos'].values,\n",
    "                df_valid['is_Source'].astype(bool).values\n",
    "\n",
    "            )\n",
    "        ]\n",
    ")\n",
    "\n",
    "df_valid['dc_flux'] = dc_flux\n",
    "df_valid['dc_sigflux'] = dc_sigflux\n",
    "\n",
    "\n",
    "\n",
    "for filt in np.unique(df['fid']):\n",
    "    mask = df_valid['fid'] == filt\n",
    "    sub = df_valid[mask]\n",
    "    plt.errorbar(\n",
    "        sub['jd'].apply(lambda x: x - 2400000.5),\n",
    "        sub['dc_flux']*1e3,\n",
    "        sub['dc_sigflux']*1e3,\n",
    "        ls='', \n",
    "        marker='o',\n",
    "        label=str(filt)\n",
    "    )\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Modified Julian Date [UTC]')\n",
    "plt.ylabel('Apparent DC flux (millijanksy)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe5d71",
   "metadata": {},
   "source": [
    "## Apparent flux for the nearest source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45ab7e",
   "metadata": {},
   "source": [
    "We create a function `flux_nr` to determine the apparent flux for the nearest source in the reference image.\n",
    "\n",
    "Please don't overlook downloading the FAnomAly package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FAnomAly.flux import flux_nr\n",
    "\n",
    "nr_flux, nr_sigflux = np.transpose(\n",
    "        [\n",
    "            flux_nr(*args, jansky=True) for args in zip(\n",
    "                df_valid['magnr'].astype(float).values,\n",
    "                df_valid['sigmagnr'].astype(float).values\n",
    "            )\n",
    "        ]\n",
    ")\n",
    "\n",
    "df_valid['nr_flux'] = nr_flux\n",
    "df_valid['nr_sigflux'] = nr_sigflux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2fa62-97c5-4073-8100-b5137aa1d73d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765880f1",
   "metadata": {},
   "source": [
    "# 5) Important steps \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54621832",
   "metadata": {},
   "source": [
    "#### Convert 'mjd' to integer to remove fractional part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e77a1e-6b69-477f-a7fb-350352c86153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['mjd'] = (df_valid['jd'] - 2400000.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282196c2-fa82-4231-8aa3-f137370b2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mjd = int(first_day - 2400000.5) #df_by_days['mjd'].min()\n",
    "max_mjd = int(last_day  - 2400000.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bab7df-ab4a-41be-a25c-0aafaaaa1497",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926d3fb",
   "metadata": {},
   "source": [
    "Here we check if we are on the case of only one pass band (there are valid data only for red filter or only for green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e475b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['source'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0559398-cf26-4cde-af12-885399aa8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_valid[df_valid['fid'] == 1]) ==0 : \n",
    "    \n",
    "    # We append two data points with zero values. This aids our algorithm without introducing any risk.\n",
    "    new_rows = pd.DataFrame({'fid': [1, 1],\n",
    "                             'mjd': [first_day - 2400000.5, last_day - 2400000.5],#[df_valid['jd'].min(), df_valid['jd'].max()],\n",
    "                             'dc_flux': [0, 0],\n",
    "                             'dc_sigflux': [0, 0],\n",
    "                             'nr_flux' : [0,0],\n",
    "                             'nr_sigflux':[0,0], \n",
    "                             'source': [0,0],\n",
    "                            })\n",
    "\n",
    "    df_valid = pd.concat([df_valid, new_rows], ignore_index=True)\n",
    "\n",
    "elif len(df_valid[df_valid['fid'] == 2]) ==0 :     \n",
    "        \n",
    "    new_rows = pd.DataFrame({'fid': [2, 2],\n",
    "                             'mjd': [first_day - 2400000.5, last_day - 2400000.5],\n",
    "                             'dc_flux': [0, 0],\n",
    "                             'dc_sigflux': [0, 0],\n",
    "                             'nr_flux' : [0,0],\n",
    "                             'nr_sigflux':[0,0],\n",
    "                             'source': [0,0],\n",
    "\n",
    "                           })\n",
    "\n",
    "    df_valid = pd.concat([df_valid, new_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf0776",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae0fe5",
   "metadata": {},
   "source": [
    "# 6) Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616249f2",
   "metadata": {},
   "source": [
    "We extract the specific columns from the valid data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83977c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['mjd', 'fid','dc_flux', 'dc_sigflux','nr_flux', 'nr_sigflux','source']\n",
    "\n",
    "combined_df = df_valid[columns_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ffe38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbd07c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b941e8",
   "metadata": {},
   "source": [
    "# 7) Data by days "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade442e",
   "metadata": {},
   "source": [
    "Here, we group the data by modified Julian date on a daily basis and by filter ID (1 for g, 2 for R), computing the average values of flux and sigma flux(for both DC and NR) using the `Weighted_Mean` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560339d1",
   "metadata": {},
   "source": [
    "#### group the data by mjd and by filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94fb10",
   "metadata": {},
   "source": [
    "#### calculate the average values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31273442",
   "metadata": {},
   "source": [
    "We extract the initial value from the 'source' column within each group. It's noteworthy that on any given day (represented by 'r/g'), there might be either missing data or available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7075b-a542-4fce-8b09-1345f410a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FAnomAly.Weighted_Mean import Weighted_Mean_general\n",
    "\n",
    "combined_df = df_valid[columns_to_keep].copy()\n",
    "df_group = combined_df.groupby(['mjd','fid'])\n",
    "\n",
    "# df_by_days = pd.DataFrame()\n",
    "df_by_days_dc = df_group.apply(Weighted_Mean_general, flux_col='dc_flux', sigflux_col='dc_sigflux')\n",
    "\n",
    "\n",
    "# Apply Weighted_Mean_general on 'nr_flux' and 'nr_sigflux'\n",
    "df_by_days_nr = df_group.apply(Weighted_Mean_general, flux_col='nr_flux', sigflux_col='nr_sigflux')\n",
    "\n",
    "df_by_days_source = df_group['source'].apply(lambda x: x.value_counts().idxmax())\n",
    "\n",
    "# Merge the results based on 'mjd' and 'fid'\n",
    "df_by_days = pd.merge(df_by_days_dc, df_by_days_nr, on=['mjd', 'fid'], suffixes=('_dc', '_nr'))\n",
    "df_by_days = pd.merge(df_by_days, df_by_days_source, on=['mjd', 'fid'])\n",
    "df_by_days.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfa48e-7c3b-477e-a8c9-39daa517ad2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_by_days.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6faa8",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40124d6",
   "metadata": {},
   "source": [
    "# 8) Compute the regression of missing days with Gaussian Process ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e0ec0-af51-4c53-9240-864729495f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "\n",
    "kernel =  ConstantKernel(constant_value=1.e-6, constant_value_bounds=(1e-12, 1)) * RBF(length_scale=5.0, length_scale_bounds=(5e-1, 5e2))\n",
    "\n",
    "def Gaussian_Process(df,flux= 'dc_flux',sigflux= 'dc_sigflux'):\n",
    "    mean_prediction = {}\n",
    "    std_prediction = {}\n",
    "    for filt in np.unique(df.fid):\n",
    "        X_train=df[df.fid==filt].mjd.values.reshape(-1, 1)\n",
    "        Y_train=np.squeeze(df[df.fid==filt][f'{flux}'].values)\n",
    "        Sig_train = np.squeeze(df[df.fid==filt][f'{sigflux}'].values)\n",
    "        X=np.arange(min_mjd,max_mjd+1,1).reshape(-1, 1)\n",
    "        # print(X)\n",
    "\n",
    "\n",
    "        if len(X_train) == 1 : \n",
    "            # If there's only one training point, create a series of mean predictions and high error bars\n",
    "            print(Y_train)\n",
    "            mean_prediction[filt] = np.full((max_mjd - min_mjd + 1,), float(Y_train))\n",
    "            std_prediction[filt] = np.full((max_mjd - min_mjd + 1,), 1 * float(Sig_train))\n",
    "            \n",
    "        else :\n",
    "            \n",
    "        \n",
    "            gaussian_process = GaussianProcessRegressor( kernel=kernel, alpha=Sig_train**2, n_restarts_optimizer=9 )\n",
    "            gaussian_process.fit(X_train, Y_train)\n",
    "            mean_prediction[filt], std_prediction[filt] = gaussian_process.predict(X, return_std=True)\n",
    "        \n",
    "        plt.errorbar(\n",
    "            X_train, Y_train, Sig_train,\n",
    "            ls='', \n",
    "            marker='o',\n",
    "            color=colordic[filt], \n",
    "            label=f\"{filt} valid difference flux\"\n",
    "        )\n",
    "    \n",
    "        plt.plot(\n",
    "            X,mean_prediction[filt], c=colordic[filt], \n",
    "            label=f\"{filt} GP prediction\"\n",
    "        )\n",
    "    \n",
    "        plt.fill_between(\n",
    "            X[:,0],mean_prediction[filt]+std_prediction[filt], mean_prediction[filt]-std_prediction[filt],color=colordic[filt], alpha=0.3\n",
    "        )\n",
    "        plt.show\n",
    "    return mean_prediction, std_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ca78c-bca5-4126-b6c6-677b2e5b6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_flux = {}\n",
    "dc_sigflux_pred = {}\n",
    "\n",
    "nr_flux_pred = {}\n",
    "nr_sigflux_pred = {}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "dc_flux_pred,dc_sigflux_pred = Gaussian_Process(df_by_days, \"dc_flux\",\"dc_sigflux\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe16bf-be27-48ba-a169-4464978fac69",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1d2f2-78dc-40a8-b46d-5dadd1cf9ddd",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e68b1-cd3c-4a77-8b4c-28d3c29848d3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda57ad3-14a8-4c61-aa74-887ea6ad213b",
   "metadata": {},
   "source": [
    "# 9) Fill the missing days ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53876f8",
   "metadata": {},
   "source": [
    "If there are missing days (without alerts in the data), we can fill these gaps with predictions from GP regression values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33983e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FAnomAly.Weighted_Mean import Weighted_Mean_all\n",
    "\n",
    "# # Create a DataFrame all_days containing a range of MJD values from the minimum to the maximum MJD found in df_by_days.\n",
    "all_days = pd.DataFrame({'mjd': range(min_mjd, max_mjd + 1)}) # + 1 i need to check this one ! \n",
    "\n",
    "df_extended = df_by_days.copy()\n",
    "\n",
    "# Inside your loop for handling missing days\n",
    "for filt in np.unique(df_extended['fid']):\n",
    "    mask = df_extended['fid'] == filt\n",
    "    data_days = df_extended[mask]['mjd']\n",
    "    sub = df_extended[mask]\n",
    "    \n",
    "    missing_days = ~all_days['mjd'].isin(data_days)\n",
    "    df_missing_days = all_days[missing_days].copy()  # Ensure a copy to avoid chain indexing issues\n",
    "    \n",
    "    true_indices = missing_days[missing_days].index.tolist()\n",
    "\n",
    "\n",
    "    df_missing_days['dc_flux'] = dc_flux_pred[filt][true_indices]\n",
    "    df_missing_days['dc_sigflux'] = dc_sigflux_pred[filt][true_indices]\n",
    "    \n",
    "    df_missing_days['fid'] = filt\n",
    "    df_missing_days['source'] = 0\n",
    "\n",
    "    # Append the missing data to df_extended\n",
    "    columns_to_keep = ['mjd', 'fid','dc_flux', 'dc_sigflux','source']\n",
    "    df_extended = pd.concat([df_extended, df_missing_days], ignore_index=True, sort=False)\n",
    "\n",
    "df_extended = df_extended[columns_to_keep].sort_values(by=['mjd', 'fid'])\n",
    "\n",
    "df_extended.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4a387",
   "metadata": {},
   "source": [
    "### plot apparent  DC and nr flux in millijanksy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "colordic = {1: 'C0', 2: 'C1'}\n",
    "filtdic = {1: 'g', 2: 'r'}\n",
    "\n",
    "\n",
    "for filt in np.unique(df_extended['fid']):\n",
    "    mask = df_extended['fid'] == filt\n",
    "    mask_missing =  df_extended['source'] == 0\n",
    "    mask_original = df_extended['source'] == 1\n",
    "    sub2 = df_extended[mask & mask_missing]\n",
    "    sub = df_extended[mask & mask_original]\n",
    "    \n",
    "    plt.errorbar(\n",
    "        sub['mjd'],\n",
    "        sub['dc_flux']*1e3, \n",
    "        sub['dc_sigflux']*1e3,\n",
    "        ls='', \n",
    "        marker='o',\n",
    "        color=colordic[filt], \n",
    "\n",
    "        label=f\"{filt} Real flux\"\n",
    "    )\n",
    "    plt.errorbar(\n",
    "        sub2['mjd'],\n",
    "        sub2['dc_flux']*1e3 ,#+ sub2['nr_flux']*1e3, \n",
    "        sub2['dc_sigflux']*1e3,# + sub2['nr_sigflux']*1e3,\n",
    "        ls='', \n",
    "        marker='x',\n",
    "        color=colordic[filt], \n",
    "\n",
    "        label=f\"{filt} GP Regression\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Modified Julian Date [UTC]')\n",
    "plt.ylabel('Apparent  DC and nr flux (millijanksy)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff4db1",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503bea0",
   "metadata": {},
   "source": [
    "# 10) Create a final dataframe to consolidate the values of this alert into a single row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7954a",
   "metadata": {},
   "source": [
    "In this dataframe, include another dataframe as a dictionary containing the values of mjd,flux, sigma, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly = pd.DataFrame()\n",
    "df_anomaly['objectId'] = [pdf_last_alert.objectId]\n",
    "df_anomaly['candid'] = [pdf_last_alert.candid]\n",
    "df_anomaly['jd'] = [pdf_last_alert.candidate['jd']]\n",
    "df_anomaly['df'] = [df_extended.to_dict()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c058243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ddc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example of how we can utilize the dataframe of the first row:\n",
    "df_test = pd.DataFrame.from_dict(df_anomaly['df'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23333851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036ff67-e0ce-4238-8b09-99d36d5fdba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f1dfd-2a9b-45f7-ab7e-01f359064a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57f88f-35cf-435e-b00a-c18efb427038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351dc40a-b5d4-4529-add3-223a06f9a31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
